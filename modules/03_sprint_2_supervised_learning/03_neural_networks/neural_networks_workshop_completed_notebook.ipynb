{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House price predictions with neural networks\n",
    "\n",
    "Small example to train a neural network to predict house prices using a simple multi-layer neural network in Keras. Data is available on [Kaggle](https://www.kaggle.com/lodhaad/house-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets read our data into memory and view the top rows using the pandas head() function\n",
    "\n",
    "data = pd.read_csv('home_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding your data is one of the most important preprocessing steps before tackling a data science problem. One of the easiest ways to look for initial correlations is to plot a correlation matrix. This can help us determine which columns are important and which columns are expendable. It is important to remember however, whilst some fields may have low correlations in their current form this does not mean they cannot be useful with the aid of some further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = data.corr()\n",
    "# Filter by price column and sort descending\n",
    "corr_mat['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do before we are ready to train a neural network is prepare our data. First we will split our labels from the main dataset and remove any unwanted fields that may confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[['price']]\n",
    "features = data.drop(['id', 'date', 'price', 'zipcode', 'yr_built', 'condition','yr_renovated', 'lat', 'long', 'sqft_lot15'], axis=1)\n",
    "\n",
    "print(features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn, one of the largest python machine learning libraries, and keras are both designed to work with pandas dataframes. Therefore, functions from both libraries can be used to aid each other. \n",
    "\n",
    "Here we use the scikit learning preprocessing class to scale our input data. This is important as large number can be problematic for neural networks. To account for this we use a StandardScaler which standardised features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_features = StandardScaler().fit_transform(features.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, labels.values, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train Size', y_train.shape, y_train.shape)\n",
    "print('Test Size', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating out model\n",
    "\n",
    "Now we need to define our model architecture and hyperparameters. The options here define not only the shape of your network but how it learns. This is where we can easily experiment with all the complex underlying mathematical principles behind neural networks.\n",
    "\n",
    "At the core of keras is the **Sequential** model. Put simply a sequential model is a step-by-step instruction for the network where the output of one line becomes the input of the next. The most important function here is the **Dense** layer. The dense layer multiplies the inputs by the weight matrix and adds the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=X_train.shape[1], kernel_initializer=\"normal\", activation='relu'))\n",
    "model.add(Dense(4, kernel_initializer=\"normal\", activation='relu'))\n",
    "model.add(Dense(4, kernel_initializer=\"normal\", activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer=\"normal\", activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set our model hyper-parameters. The key parameters to decide here are the [loss function](https://keras.io/losses/), [optimiser](https://keras.io/optimizers/), [epoch and batch](https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean). Understanding each of these and experimenting with different combinations can is the key to a successful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "lr = 0.3\n",
    "\n",
    "# Set optimiser\n",
    "opt = optimizers.Adam(lr=lr)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "\n",
    "# Set to variable if you want to store training statistics\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import a plot library to visualise statistics of our model training. This can be very useful for determining if models are still imrpoving, have already converged or are over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "\n",
    "Once you have trained your model its performance needs to be evaluated.  The easiest way to do this is first run your model on your entire test dataset that we set aside earlier. Once we have a list of our results we can use a simple loop to iterate through the results and compare each result with the actual value. \n",
    "\n",
    "**Note:** Remember to calcualte an inverse of the scalar we applied earlier to scale the numbers back to there original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "print(\"Total error: $%.2f\" %mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Both keras and scikit-learn are designed to take numpy arrays and pandas data frames as inputs. Therefore we can easily pass our training data into a range of scikit-learn regression models such as; [Linear](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) or [Support Vector Machine](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "l_predictions = regr.predict(X_test)\n",
    "\n",
    "l_mae = mean_absolute_error(y_test, l_predictions)\n",
    "\n",
    "print(\"Total error: $%.2f\" %l_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
